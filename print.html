<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Getting started with Apache Airflow on Cloud Composer</title>
        <meta name="robots" content="noindex" />
        <!-- Custom HTML head -->
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">
        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">
        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="index.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li class="chapter-item expanded "><a href="apache-airflow-and-its-concepts.html"><strong aria-hidden="true">2.</strong> Apache Airflow and its concepts</a></li><li class="chapter-item expanded "><a href="google-cloud-composer/index.html"><strong aria-hidden="true">3.</strong> Google Cloud Composer</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="google-cloud-composer/benefits.html"><strong aria-hidden="true">3.1.</strong> Benefits</a></li><li class="chapter-item expanded "><a href="google-cloud-composer/architecture.html"><strong aria-hidden="true">3.2.</strong> Architecture</a></li><li class="chapter-item expanded "><a href="google-cloud-composer/version-support-and-deprecation.html"><strong aria-hidden="true">3.3.</strong> Version support and deprecation</a></li><li class="chapter-item expanded "><a href="google-cloud-composer/limitations.html"><strong aria-hidden="true">3.4.</strong> Limitations</a></li></ol></li><li class="chapter-item expanded "><a href="build-airflow-pipelines-on-composer/index.html"><strong aria-hidden="true">4.</strong> Build Airflow pipelines on Composer</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="build-airflow-pipelines-on-composer/deploy-a-composer-environment.html"><strong aria-hidden="true">4.1.</strong> Deploy a Composer environment</a></li><li class="chapter-item expanded "><a href="build-airflow-pipelines-on-composer/a-tour-of-airflow-ui.html"><strong aria-hidden="true">4.2.</strong> A tour of the Airflow UI</a></li><li class="chapter-item expanded "><a href="build-airflow-pipelines-on-composer/write-the-dags/index.html"><strong aria-hidden="true">4.3.</strong> Write the DAGs</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="build-airflow-pipelines-on-composer/write-the-dags/first_dag.html"><strong aria-hidden="true">4.3.1.</strong> First DAG</a></li><li class="chapter-item expanded "><a href="build-airflow-pipelines-on-composer/write-the-dags/context_manager_dag.html"><strong aria-hidden="true">4.3.2.</strong> Context manager DAG</a></li><li class="chapter-item expanded "><a href="build-airflow-pipelines-on-composer/write-the-dags/parallel_tasks_dag.html"><strong aria-hidden="true">4.3.3.</strong> Parallel tasks DAG</a></li><li class="chapter-item expanded "><a href="build-airflow-pipelines-on-composer/write-the-dags/dynamic_tasks_dag.html"><strong aria-hidden="true">4.3.4.</strong> Dynamic tasks DAG</a></li><li class="chapter-item expanded "><a href="build-airflow-pipelines-on-composer/write-the-dags/branching_dag.html"><strong aria-hidden="true">4.3.5.</strong> Branching DAG</a></li><li class="chapter-item expanded "><a href="build-airflow-pipelines-on-composer/write-the-dags/xcoms_dag.html"><strong aria-hidden="true">4.3.6.</strong> Xcoms DAG</a></li><li class="chapter-item expanded "><a href="build-airflow-pipelines-on-composer/write-the-dags/python_operator_and_taskflow_dag.html"><strong aria-hidden="true">4.3.7.</strong> PythonOperator and Taskflow Dag</a></li><li class="chapter-item expanded "><a href="build-airflow-pipelines-on-composer/write-the-dags/custom_operator_dag.html"><strong aria-hidden="true">4.3.8.</strong> Custom operator DAG</a></li></ol></li><li class="chapter-item expanded "><a href="case-study/index.html"><strong aria-hidden="true">4.4.</strong> Case study: generate nudges</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="case-study/create-nudges-dag.html"><strong aria-hidden="true">4.4.1.</strong> Create nudges DAG</a></li><li class="chapter-item expanded "><a href="case-study/testing.html"><strong aria-hidden="true">4.4.2.</strong> Testing</a></li><li class="chapter-item expanded "><a href="case-study/ci-and-cd.html"><strong aria-hidden="true">4.4.3.</strong> CI and CD</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="set-up-and-run-airflow-locally.html"><strong aria-hidden="true">5.</strong> Set up and run Airflow locally</a></li><li class="chapter-item expanded "><a href="what-next.html"><strong aria-hidden="true">6.</strong> What's next?</a></li><li class="spacer"></li><li class="chapter-item expanded affix "><a href="misc/contributors.html">Contributors</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Getting started with Apache Airflow on Cloud Composer</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<p>This repository aims to provide a short course about the basics of <a href="https://airflow.apache.org/">Apache Airflow</a> and how to run it on <a href="https://cloud.google.com/composer">Google Cloud Composer</a> on Google Cloud.</p>
<p>The repository uses <a href="https://rust-lang.github.io/mdBook/">mdBook</a> to create a mini-book tutorial. You can find the source code of this mini-book under the <em>code</em> folder.</p>
<h2 id="learning-goals"><a class="header" href="#learning-goals">Learning Goals</a></h2>
<p>Through this course, you will experience working with Airflow data pipelines on Google Cloud Composer. You will become familiar with the following topics:</p>
<ul>
<li>Apache Airflow and its concepts</li>
<li>Understanding of Google Cloud Composer</li>
<li>Setting up Airflow</li>
<li>Building Airflow pipelines</li>
</ul>
<h2 id="access-the-mini-book"><a class="header" href="#access-the-mini-book">Access the mini-book</a></h2>
<p>This mini-book has been deployed to GitHub pages, you can access it from <a href="https://doitintl.github.io/doit-composer-airflow-training/">here</a>.</p>
<p>You can also run it locally:</p>
<ol>
<li>Clone the repository</li>
<li>Install mdBook following the <a href="https://github.com/rust-lang/mdBook#installation">guide</a></li>
<li>Run <code>make serve</code></li>
<li>In your browser, navigate to <code>http://localhost:3000</code></li>
</ol>
<!-- textlint-disable en-capitalization,rousseau -->
<p><img src="mdbook-website.png" alt="mdBook website" /></p>
<!-- textlint-enable -->
<h2 id="contribution"><a class="header" href="#contribution">Contribution</a></h2>
<p>If you'd like to get involved, either fixing a grammar mistake or adding a new chapter, feel free to send a pull request.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="apache-airflow-and-its-concepts"><a class="header" href="#apache-airflow-and-its-concepts">Apache Airflow and its concepts</a></h1>
<h2 id="what-is-airflow"><a class="header" href="#what-is-airflow">What is Airflow</a></h2>
<p>Airflow is a platform to programmatically create, schedule, and monitor workflows.</p>
<p>You can use Airflow to create workflows as <a href="https://en.wikipedia.org/wiki/Directed_acyclic_graph">Directed Acyclic Graphs</a> (DAGs) of tasks. The Airflow scheduler executes your tasks on various workers while following the specified dependencies. Rich command-line utilities make performing complex surgeries on DAGs a snap. The rich user interface helps to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.</p>
<h2 id="quick-peek"><a class="header" href="#quick-peek">Quick peek</a></h2>
<p><img src="https://airflow.apache.org/docs/apache-airflow/stable/_images/airflow.gif" alt="Airflow" /></p>
<h2 id="why-airflow-is-popular"><a class="header" href="#why-airflow-is-popular">Why Airflow is popular</a></h2>
<ul>
<li>You can define workflows as Python code, so that they:
<ul>
<li>Are more flexible</li>
<li>Are testable</li>
<li>Are reusable</li>
<li>Can access the whole Python echo system</li>
</ul>
</li>
<li>Battery included platform
<ul>
<li>Airflow provides libraries to connect
<ul>
<li>Popular database: MySQL, Postgres, MongoDB, Oracle, SQL Server, Snowflake and BigQuery</li>
<li>Services: Databricks, Datadog, ElasticSearch, Jenkins, Salesforce, SendGrid, Slack and Zendesk</li>
</ul>
</li>
</ul>
</li>
<li>You can deploy Airflow to public cloud platforms: <em>Amazon Web Services</em> (AWS), Azure, and <em>Google Cloud Platform</em> (GCP)</li>
<li>Informational and feature-rich UI to visualize workflows' status, monitor progress, troubleshoot issues, trigger, and re-trigger workflows and tasks in them</li>
</ul>
<h2 id="beyond-the-horizon"><a class="header" href="#beyond-the-horizon">Beyond the Horizon</a></h2>
<p>Airflow is <em>not</em> a data streaming solution. Workflows are expected to be mostly static or slowly changing. Below are a few example use cases of it:</p>
<ul>
<li><code>Daily</code>—Load batch files from different databases to a reporting database</li>
<li><code>Daily/Weekly/Monthly</code>—Generate and deliver reports to stakeholders</li>
<li><code>Daily</code>—Re-train machine learning models with fresh data</li>
<li><code>Hourly</code>—Back up data from a database</li>
<li><code>Hourly</code>—Generate and send recommended products to customers based on customers activities - think spam emails you get from eBay</li>
<li><code>On-demand (triggered)</code>—Send registration emails to newly registered customers</li>
</ul>
<h2 id="airflow-concepts"><a class="header" href="#airflow-concepts">Airflow concepts</a></h2>
<ul>
<li><code>DAG</code>—A DAG is a collection of tasks and describe how to run a workflow written in Python. A pipeline is designed as a directed acyclic graph, in which the tasks can be executed independently. Then these tasks are combined logically as a graph.</li>
<li><code>Task</code>—A Task defines a unit of work within a DAG; it is represented as a <!-- textlint-disable terminology -->node<!-- textlint-enable --> in the DAG graph. Each task is an implementation of an Operator, for example, a PythonOperator to execute some Python code or a BashOperator to run a Bash command. After an operator is instantiated, it's called a <em>task</em>.</li>
<li><code>Task instance</code>—A task instance represents a specific run of a task characterized by a DAG, a task, and a point (execution_date).</li>
<li><code>Operators</code>—Operators are atomic components in a DAG describing a single task in the pipeline. They determine what gets done in that task when a DAG runs. Airflow provides operators for common tasks. It is extendable so that you can define your own custom operators.</li>
<li><code>Sensors</code>—Sensors are special operators that repeatedly run until the predefined condition is fulfilled. For example a file sensor can wait until the file lands, then continue the workflow</li>
<li><code>Hooks</code>—Provide a uniform interface to access external services like <em>Google Cloud Storage</em> (GCS), BigQuery, PubSub, etc. Hooks are the building blocks for Operators/Sensors to interact with external services.</li>
<li><code>DAG run</code>—when a DAG is triggered, it is called a DAG run. It represents the instance of the workflow</li>
<li><code>Scheduler</code>—Airflow scheduler monitors all tasks and DAGs, then triggers the task instances once their dependencies are complete</li>
<li><code>Executor</code>—Airflow Executors are the mechanism by which task instances get to run. The most popular Executor is Celery Executor</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="google-cloud-composer"><a class="header" href="#google-cloud-composer">Google Cloud Composer</a></h1>
<p>Google Cloud Composer is a fully managed workflow orchestration service built on Apache Airflow. It is the recommended way to run Airflow on Google Cloud.</p>
<p>In this section, we will talk about its benefits, architecture, support, and limitations.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="benefits"><a class="header" href="#benefits">Benefits</a></h1>
<p>As a fully managed Airflow service, Cloud Composer has the below benefits:</p>
<ul>
<li>Fully managed workflow orchestration
<ul>
<li>Cloud Composer's managed nature and Apache Airflow compatibility allow you to focus on authoring, scheduling and monitoring your workflows as opposed to provisioning resources</li>
</ul>
</li>
<li>Integrates with other Google Cloud products
<ul>
<li>End-to-end integration with Google Cloud products, including BigQuery, Dataflow, Dataproc, Datastore, Cloud Storage, Pub/Sub, and AI Platform, gives users the freedom to fully orchestrate their pipelines</li>
</ul>
</li>
<li>Supports hybrid and multi-cloud
<ul>
<li>Author, schedule, and monitor your workflows through a single orchestration tool—whether your pipeline lives on-premises, in multiple clouds, or entirely within Google Cloud</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="cloud-composer-architecture"><a class="header" href="#cloud-composer-architecture">Cloud Composer Architecture</a></h1>
<p>In this section, we will talk about the architecture of Cloud Composer.</p>
<h2 id="composer-components"><a class="header" href="#composer-components">Composer components</a></h2>
<p>A Cloud Composer environment contains multiple components, below are some of the important ones:</p>
<ul>
<li>Airflow Webserver
<ul>
<li>It is a visual management interface powered by <a href="https://flask-appbuilder.readthedocs.io/">Flask-AppBuilder</a></li>
<li>It provides the ability to
<ul>
<li>View status of DAGs and their status</li>
<li>Display logs from each DAG and worker</li>
<li>Act on the DAG status (pause, unpause, trigger)</li>
<li>Configure Airflow, such as variables, connections, etc</li>
<li>Each view the code of DAGs</li>
</ul>
</li>
</ul>
</li>
<li>Airflow scheduler
<ul>
<li>It monitors all tasks and DAGs, then triggers the task instances once their dependencies are complete</li>
<li>Behind the scenes, the scheduler spins up a subprocess, which monitors and stays in sync with all DAGs in the specified DAG directory. Once per minute, by default, the scheduler collects DAG parsing results and checks whether any active task can be triggered</li>
</ul>
</li>
<li>Airflow worker
<ul>
<li>It executes individual tasks from DAGs by taking them from the Redis queue.</li>
</ul>
</li>
<li>Airflow database hosted on CloudSQL
<ul>
<li>It stores all the information of Airflow, including DAGs, task information, configs, etc</li>
</ul>
</li>
<li>Airflow buckets
<ul>
<li>When you create an environment, Cloud Composer creates a Cloud Storage bucket and associates the bucket with your Composer environment.</li>
<li>You can store custom code in the Cloud Storage bucket, including DAGs, plugins, and other resources.</li>
<li>Behind the scene, Composer uses <a href="https://github.com/GoogleCloudPlatform/gcsfuse">gcsfuse</a> to sync all the content to Airflow workers and webserver.</li>
<li>You can find a Google <a href="https://cloud.google.com/composer/docs/concepts/cloud-storage#folders_in_the_bucket">document</a> that shows the mapping list between the folders in the bucket to Airflow folders.</li>
</ul>
</li>
<li>Redis queue
<ul>
<li>It holds a queue of individual tasks from your DAGs. Airflow schedulers fill the queue; Airflow workers take their tasks from it</li>
</ul>
</li>
</ul>
<h2 id="customer-and-tenant-projects"><a class="header" href="#customer-and-tenant-projects">Customer and tenant projects</a></h2>
<p>Cloud Composer runs on both customer and tenant (Google-managed) projects; Composer distributes the environment's resources between a tenant and a customer project. The resources are deployed to different projects because of varying environment setups.</p>
<p>However, regardless of which environment setup and Composer versions you choose, there is:</p>
<ul>
<li>A <em>Google Kubernetes Engine</em> (GKE) cluster in the customer project that runs the Airflow workloads</li>
<li>A CloudSQL instance on the tenant project, and</li>
<li>A Cloud Storage bucket to store DAGs and plugins</li>
</ul>
<h2 id="different-composer-setups"><a class="header" href="#different-composer-setups">Different Composer setups</a></h2>
<p>Below are the different setups of Composer 1 and 2</p>
<h3 id="composer-1"><a class="header" href="#composer-1">Composer 1</a></h3>
<p><a href="https://cloud.google.com/composer/docs/concepts/architecture#public-ip">Public IP</a></p>
<p><img src="https://cloud.google.com/composer/docs/images/composer-1-public-ip-architecture.svg" alt="Public IP" /></p>
<p><a href="https://cloud.google.com/composer/docs/concepts/architecture#private-ip">Private IP</a></p>
<p><img src="https://cloud.google.com/composer/docs/images/composer-1-private-ip-architecture.svg" alt="Private IP" /></p>
<!-- textlint-disable stop-words -->
<p><a href="https://cloud.google.com/composer/docs/concepts/architecture#private-ip-drs">Private IP with DRS</a></p>
<p>If the <a href="https://cloud.google.com/resource-manager/docs/organization-policy/org-policy-constraints">Domain Restricted Sharing (DRS) organizational policy</a> is turned on in the project, then Cloud Composer uses the Private IP with DRS environment architecture.</p>
<!-- textlint-enable -->
<p><img src="https://cloud.google.com/composer/docs/images/composer-1-private-ip-drs-architecture.svg" alt="Private IP with DRS" /></p>
<h3 id="composer-2"><a class="header" href="#composer-2">Composer 2</a></h3>
<p><a href="https://cloud.google.com/composer/docs/composer-2/environment-architecture#public-ip">Public IP</a></p>
<p><img src="https://cloud.google.com/composer/docs/images/composer-2-public-ip-architecture.svg" alt="Public IP" /></p>
<p><a href="https://cloud.google.com/composer/docs/composer-2/environment-architecture#private-ip">Private IP</a></p>
<p><img src="https://cloud.google.com/composer/docs/images/composer-2-private-ip-architecture.svg" alt="Private IP" /></p>
<h3 id="comparison-of-composer-1-and-2"><a class="header" href="#comparison-of-composer-1-and-2">Comparison of Composer 1 and 2</a></h3>
<p>Composer 2 is the future version of Composer. Most of the users are still using Composer 1 because Composer 1 is still the default. We can expect Composer 1 to be retired soon, and Composer 2 will be the only supported one.
From the above architecture diagrams, there are a few things that are some differences between these two versions:</p>
<ol>
<li>The significant improvement of Composer 2 is <strong>autoscaling</strong>; it leverages <a href="https://cloud.google.com/kubernetes-engine/docs/concepts/autopilot-overview">GKE Autopilot</a> feature, meaning the Composer environment can be automatically scaled up to handle more workloads and scale down to reduce cost.</li>
<li>Composer 2 only supports Airflow 2 and Python 3.8+. Suppose there is a requirement of using Python 2(please consider upgrade to Python 3) and Airflow 1.10.*, Composer 1 is the only option.</li>
<li>In Composer 2, the Airflow Webserver is moved to GKE from App Engine running on the tenant project.</li>
</ol>
<p>You can find more details of the differences between the two Composer versions from <a href="https://cloud.google.com/composer/docs/composer-2/composer-versioning-overview#major-versions">here</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="version-support-and-deprecation"><a class="header" href="#version-support-and-deprecation">Version support and deprecation</a></h1>
<h2 id="versions"><a class="header" href="#versions">Versions</a></h2>
<p>There are two versions of Cloud Composer, Composer 1 and 2. You can find the full list of Composer versions <a href="https://cloud.google.com/composer/docs/concepts/versioning/composer-versions">here</a>.</p>
<p>A typical version looks like this:</p>
<blockquote>
<p>composer-1.17.5-airflow-2.1.4</p>
</blockquote>
<p>In the version identifier, <code>1.17.5</code> is the version of Composer, while <code>2.1.4</code> is the version of Airflow.</p>
<p>You can consider each <code>Composer</code> version contains all the close-sourced Google deployments and hosting code and each <code>Airflow</code> version contains the open-sourced code from Apache Airflow code that is hosted on GitHub <a href="https://github.com/apache/airflow">repository</a>.</p>
<h2 id="understanding-composer-and-airflow-versions"><a class="header" href="#understanding-composer-and-airflow-versions">Understanding Composer and Airflow versions</a></h2>
<p>While there are two versions of Cloud Composer and two versions of Apache Airflow, each version of Composer <em>does not</em> directly map to a version of Airflow.</p>
<p><strong>Composer v1 has support for both Airflow v1 and v2, while Composer v2 only supports Airflow v2.</strong></p>
<p>As mentioned in Section 3.2, the primary difference between Composer 1 and 2 is the involvement of GKE Autopilot. Composer 2 takes advantage of GKE Autopilot for autoscaling. Composer 1, by contrast, does not support autoscaling.</p>
<p>The differences of Airflow 1 and 2 are more about additional/improved functionality detailed at length <a href="https://airflow.apache.org/blog/airflow-two-point-oh-is-here/">here</a>. The most anticipated new features in Airflow 2 include: Easier to author DAGs, massive scheduler performance improvements, high-availability support for the job scheduler, and an improved UI.</p>
<h2 id="support-and-deprecation"><a class="header" href="#support-and-deprecation">Support and deprecation</a></h2>
<h3 id="composer"><a class="header" href="#composer">Composer</a></h3>
<p>Cloud Composer version support is defined as follows:</p>
<ul>
<li>0-12 months from the release date: Cloud Composer environments running these versions are fully supported.</li>
<li>12-18 months from the release date: Cloud Composer environments running these versions are unsupported except to notify customers about security issues.</li>
<li>18+ months from the release date: Cloud Composer environments running these versions are unsupported and entirely user-managed.</li>
</ul>
<h3 id="airflow"><a class="header" href="#airflow">Airflow</a></h3>
<p>A particular version of Apache Airflow found in Cloud Composer is <em>not</em> always an exact match of the corresponding version in upstream Airflow because Cloud Composer uses a patched version of Airflow.
<a href="https://github.com/GoogleCloudPlatform/composer-airflow/tree/2.0.2">This repository</a> contains the code for every patched version of Airflow used in Composer, and it is useful for:</p>
<ul>
<li>Finding out if a specific commit from the <a href="https://github.com/apache/airflow">Airflow open-source project</a> is in the Composer version</li>
<li>Reproducing issues locally</li>
<li>Check how an Operator/Sensor/Hook looks like in the Composer version</li>
</ul>
<p>When deep-dive troubleshooting an Airflow issue from Cloud Composer, you may want to look into Google's official composer-airflow repository.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="limitations"><a class="header" href="#limitations">Limitations</a></h1>
<h2 id="limited-access-to-underline-systems"><a class="header" href="#limited-access-to-underline-systems">Limited access to underline systems</a></h2>
<p>Being a hosted service, Google manages all the infrastructure. As a customer, you don't have access to the underline systems and the database. Most of the time, it is excellent and worry-free.</p>
<p>However, it can be a pain when you bump into any issue that requires more deep investigations.</p>
<h2 id="painful-upgrade-process"><a class="header" href="#painful-upgrade-process">Painful upgrade process</a></h2>
<p>Each Composer version has one year of Google support. After that, it is recommended to upgrade to a newer supported version.
To upgrade the Composer version, Google provides a beta <a href="https://cloud.google.com/composer/docs/how-to/managing/upgrading">feature</a>. However, it only works if the old Composer version is not that old. Some Cloud Composer users are using out-of-support versions - some environments are more than two years old. The managed upgrading process that Google provides would most likely fail.</p>
<p>In this scenario, it is recommended that the customer creates a new Composer environment and migrate all the existing workflows to it.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="build-airflow-pipelines-on-composer"><a class="header" href="#build-airflow-pipelines-on-composer">Build Airflow pipelines on Composer</a></h1>
<p>In this chapter, we will deploy a Cloud Composer environment, explore the Airflow UI, and write a few DAGs. Towards the end, we will follow a case study, analyzing the business requirements that will inform creation of an example Airflow pipeline.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="deploy-a-composer-environment"><a class="header" href="#deploy-a-composer-environment">Deploy a Composer environment</a></h1>
<p>Cloud Composer can be deployed via Console, gcloud cli, API, and Terraform. You can find the detailed instructions from Google's <a href="https://cloud.google.com/composer/docs/how-to/managing/creating">document</a>.</p>
<p>In this tutorial, we will create a public IP Composer 1 (Composer 2 only recently became Generally Available) using an Airflow 2 environment with <strong>gcloud</strong>.</p>
<p><em>The bash script below can be found at: <code>code/deploy-composer.sh</code></em></p>
<pre><code class="language-bash">#! /bin/bash

# Variables to set
PROJECT_ID=$(gcloud config get-value project)
#echo &quot;Setting default GCP project to ${PROJECT_ID}&quot;
#gcloud config set project ${PROJECT_ID}
COMPOSER_SERVICE_ACCOUNT_NAME=&quot;composer-training&quot;

echo &quot;Creating a service account for Composer environment to use&quot;
gcloud iam service-accounts create &quot;${COMPOSER_SERVICE_ACCOUNT_NAME}&quot; \
    --description=&quot;composer training service account&quot; \
    --display-name=&quot;composer training service account&quot;

COMPOSER_SERVICE_ACCOUNT_ID=&quot;${COMPOSER_SERVICE_ACCOUNT_NAME}@${PROJECT_ID}.iam.gserviceaccount.com&quot;
echo &quot;Created service account: ${COMPOSER_SERVICE_ACCOUNT_ID}&quot;

# https://cloud.google.com/composer/docs/how-to/access-control#service-account
echo &quot;Adding composer.worker roles to the service account&quot;
gcloud projects add-iam-policy-binding &quot;${PROJECT_ID}&quot; \
    --member=&quot;serviceAccount:${COMPOSER_SERVICE_ACCOUNT_ID}&quot; \
    --role=&quot;roles/composer.worker&quot;

echo &quot;Adding GCS and BigQuery admin roles for the service account to run this tutorial&quot;
gcloud projects add-iam-policy-binding &quot;${PROJECT_ID}&quot; \
    --member=&quot;serviceAccount:${COMPOSER_SERVICE_ACCOUNT_ID}&quot; \
    --role=&quot;roles/storage.admin&quot;
gcloud projects add-iam-policy-binding &quot;${PROJECT_ID}&quot; \
    --member=&quot;serviceAccount:${COMPOSER_SERVICE_ACCOUNT_ID}&quot; \
    --role=&quot;roles/bigquery.admin&quot;

echo &quot;Enabling Composer API, this will take a few minutes...&quot;
gcloud services enable composer.googleapis.com

echo &quot;Creating Cloud Composer, this will take ~25 minutes...&quot;
gcloud composer environments create composer-training \
    --location us-central1 \
    --node-count 3 \
    --scheduler-count 1 \
    --disk-size 100 \
    --machine-type n1-standard-1 \
    --cloud-sql-machine-type db-n1-standard-2 \
    --web-server-machine-type composer-n1-webserver-2 \
    --image-version &quot;composer-1.17.5-airflow-2.1.4&quot; \
    --service-account &quot;${COMPOSER_SERVICE_ACCOUNT_ID}&quot; \
    --zone us-central1-c
</code></pre>
<p>After running the deployment scripts, verify:</p>
<ol>
<li>
<p>From <a href="https://console.cloud.google.com/iam-admin">Identity and Access Management</a> (IAM) UI, a service account named <code>composer-training@${PROJECT_ID}.iam.gserviceaccount.com</code> has been created, and it has Cloud Storage and BigQuery Admin roles.</p>
<p><img src="build-airflow-pipelines-on-composer/composer-service-account.png" alt="Composer service account" /></p>
</li>
<li>
<p>From <a href="https://console.cloud.google.com/composer">Composer</a> UI that a Composer environment named <code>composer-training</code> has been created.</p>
<p><img src="build-airflow-pipelines-on-composer/composer-environment.png" alt="Composer environment" /></p>
</li>
</ol>
<p>Lastly, click on the Airflow webserver to open the Airflow UI. Note that the Composer 1 environment we deployed uses Airflow 2 - the UI might look <a href="https://airflow.apache.org/docs/apache-airflow/1.10.15/ui.html">slightly different</a> if you deployed an Airflow 1 version on Composer 1.</p>
<p><img src="build-airflow-pipelines-on-composer/airflow-webserver.png" alt="Airflow webserver" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="a-tour-of-the-airflow-ui"><a class="header" href="#a-tour-of-the-airflow-ui">A tour of the Airflow UI</a></h1>
<p>The Airflow UI allows you to monitor and troubleshoot your data pipelines. It can also be helpful to manage Airflow cluster, and you can view and set up Airflow's:</p>
<ul>
<li>Variables</li>
<li>Configurations</li>
<li>Connections</li>
<li>Plugins</li>
<li>Pools</li>
<li>XComs</li>
</ul>
<h2 id="dags-view"><a class="header" href="#dags-view">DAGs view</a></h2>
<p>The default page of Airflow UI is the DAGs view. It lists all the DAGs that are available on this Airflow instance. You can see exactly how many tasks succeeded, failed, or are still running.</p>
<p><img src="https://airflow.apache.org/docs/apache-airflow/stable/_images/dags.png" alt="DAGs view" /></p>
<p>From here, you can manually trigger the DAGs by clicking the <em>Play</em> button. This can be beneficial if you have a scheduled DAG that failed and would like to re-run it:</p>
<p><img src="build-airflow-pipelines-on-composer/trigger-dag.png" alt="Trigger dag" /></p>
<p>Note that there is also an option to pass parameters (a JSON blob) when triggering dags. This enabled flexibility when running a DAG:</p>
<p><img src="build-airflow-pipelines-on-composer/trigger-dag-with-parameters.png" alt="Trigger dag with parameters" /></p>
<h2 id="individual-dag-views"><a class="header" href="#individual-dag-views">Individual DAG views</a></h2>
<p>After a short tour of the overall DAG view, let's look at some individual DAG views.</p>
<p>There are three kinds of individual DAG views: Tree view, Graph view, and Calendar view.</p>
<h3 id="tree-view"><a class="header" href="#tree-view">Tree view</a></h3>
<p>After clicking the DAG name, the tree view is the default view.</p>
<p>A tree representation of the DAG that spans across time. You can tell each task's status in different colors.</p>
<p><img src="https://airflow.apache.org/docs/apache-airflow/stable/_images/tree.png" alt="Tree view" /></p>
<h3 id="graph-view"><a class="header" href="#graph-view">Graph view</a></h3>
<p>The graph view is my favorite view when I troubleshoot the issue with Airflow UI. It visualizes your DAG's dependencies and their current status for a specific run.</p>
<p><img src="https://airflow.apache.org/docs/apache-airflow/stable/_images/graph.png" alt="Graph view" /></p>
<p>After clicking the task, A pop-up shows, you can conduct a few actions from the pop-up dialog for this specific task instance.</p>
<p><img src="build-airflow-pipelines-on-composer/task-instance.png" alt="Task instance" /></p>
<p>One of the most valuable buttons here is <code>Log</code> because it shows you the stack trace.</p>
<p>When a DAG fails, you typically begin the troubleshooting process by:</p>
<ol>
<li>Opening the Airflow UI</li>
<li>Clicking the failed DAG</li>
<li>Going to <code>Graph view</code></li>
<li>Clicking the failed task (marked in <code>red</code>)</li>
<li>On the task instance pop-up dialog, click <code>Log</code> to check the logs. The stacktrace is here:</li>
</ol>
<p><img src="build-airflow-pipelines-on-composer/airflow-ui-logs.png" alt="Airflow-ui-logs" /></p>
<p>If the error is caused by some solvable issue (for example, the input file didn't arrive), after fixing the issue, go back to the task instance pop-up dialog and click the <code>Clear</code> button. This will clear the status of the current task instance and it will be re-run. If there are following tasks after it, they will be re-run as well.</p>
<h3 id="calendar-view"><a class="header" href="#calendar-view">Calendar view</a></h3>
<p>The calendar view gives you an overview of your entire DAG's history over months, or even years. Letting you quickly see trends of the overall success/failure rate of runs overtime.</p>
<p>It can be pretty handy to get an idea of how stable the DAG is. For example, if your DAG is waiting for files to land on a Cloud Storage bucket, this can tell you if they have been late in the past X days:</p>
<p><img src="https://airflow.apache.org/docs/apache-airflow/stable/_images/calendar.png" alt="Calendar view" /></p>
<h2 id="admin-pages"><a class="header" href="#admin-pages">Admin pages</a></h2>
<h3 id="variable-view"><a class="header" href="#variable-view">Variable view</a></h3>
<p>The variable view allows you to list, create, edit or delete the key-value pairs used in Airflow pipelines.</p>
<p>Value of a variable will be hidden if the key contains any words in (<code>password</code>, <code>secret</code>, <code>passwd</code>, <code>authorization</code>, <code>api_key</code>, <code>apikey</code>, <code>access_token</code>) by default, but can be configured to show in plain-text.</p>
<p>It is generally <em>not</em> recommended to store sensitive data like passwords and API keys within Airflow variables. On Google Cloud, Cloud Composer has native integration with <a href="https://cloud.google.com/secret-manager/docs">Google Cloud Secret Manager</a>; you can find a detailed guide on how to use it <a href="https://cloud.google.com/composer/docs/secret-manager">here</a>.</p>
<p>Airflow variables can be helpful to store insensitive environment variables. For example, database connection strings, API endpoints, GCS bucket names or paths, etc.</p>
<p><img src="https://airflow.apache.org/docs/apache-airflow/stable/_images/variable_hidden.png" alt="Variable view" /></p>
<h3 id="configuration-view"><a class="header" href="#configuration-view">Configuration view</a></h3>
<p>The Configuration view page displays all the Airflow configurations, including the ones in <code>airflow.cfg</code> and <code>Running Configuration</code>.</p>
<p><img src="build-airflow-pipelines-on-composer/airflow-ui-config-cfg.png" alt="Airflow-cfg" /></p>
<p><img src="build-airflow-pipelines-on-composer/airflow-ui-config-running.png" alt="Airflow-running-configuration" /></p>
<p>Note that default configurations in <code>airflow.cfg</code> are overwritten by the environment variables. As per <a href="https://cloud.google.com/composer/docs/overriding-airflow-configurations">Google document</a>, these environment variables can be set from cloud console, gcloud cli, API, and Terraform.</p>
<h3 id="connection-view"><a class="header" href="#connection-view">Connection view</a></h3>
<p>In the context of Airflow, a <code>connection</code> stores information such as hostname, port, login, and passwords to other systems and services. The pipeline code you will author will reference the <code>conn_id</code> of the Connection objects.</p>
<p>On the connection view page, connections information is shown and can be edited:</p>
<p><img src="https://airflow.apache.org/docs/apache-airflow/stable/_images/connections.png" alt="Connection view" /></p>
<p>In the <a href="build-airflow-pipelines-on-composer/deploy-a-composer-environment.html">last chapter</a>, we deployed a Cloud Composer environment with a service account. In the Airflow pipelines in Composer, by default, they will use this service account to authorize all the Google Cloud API calls. This saves time setting up Google Cloud connection on the Airflow configuration view page.</p>
<h3 id="xcoms-view"><a class="header" href="#xcoms-view">XComs view</a></h3>
<p><a href="https://airflow.apache.org/docs/apache-airflow/stable/concepts/xcoms.html">XComs</a> (short for <em>cross-communications</em>) is a mechanism that let tasks talk to each other, as by default, Tasks are entirely isolated and may be running on completely different machines.</p>
<p>XComs view page shows all the values that are stored as XComs. It can be helpful when troubleshooting failed DAGs that use XComs to pass the values from one task to another:</p>
<p><img src="build-airflow-pipelines-on-composer/airflow-ui-xcoms.png" alt="XComs" /></p>
<h2 id="summary"><a class="header" href="#summary">Summary</a></h2>
<p>Airflow UI can be pretty valuable for:</p>
<ul>
<li>Data engineers to troubleshoot DAG failures</li>
<li>Managers to quickly get an idea about scheduled workloads</li>
<li>Administrators to set up Variables and Connections. However, these are better done via deployment scripts via gcloud cli, API, or Terraform</li>
</ul>
<p>That's all for the UI bit, time to write and run some DAGs.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="write-the-dags"><a class="header" href="#write-the-dags">Write the DAGs</a></h1>
<p>In this chapter, we will learn how to write the DAGs. We will use some of the fundamental Airflow concepts we learned to create these pipelines.</p>
<h2 id="basics-of-writing-a-dag"><a class="header" href="#basics-of-writing-a-dag">Basics of writing a DAG</a></h2>
<p>Airflow uses Python to define DAGs. A DAG file is nothing but a standard Python file.</p>
<p>To define a DAG, typically, there are five steps:</p>
<ol>
<li>Import Python modules</li>
<li>Define default args
<ul>
<li>These args will get passed on to each Operator/Sensor</li>
<li>Args can be overridden on a per-task basis during Operator/Sensor initialization</li>
</ul>
</li>
<li>Instantiate DAG / Create a DAG object</li>
<li>Define tasks
<ul>
<li>A task can be an instance of an Operator or a Sensor</li>
</ul>
</li>
<li>Define task sequence and dependencies</li>
</ol>
<p>Let's continue and write our first DAG.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="first-dag"><a class="header" href="#first-dag">First DAG</a></h1>
<p>Let's start with a basic DAG with two tasks: <code>start</code> and <code>finish</code>.</p>
<p>Let's assume this DAG runs at 03:00 daily. Both of the tasks in the DAG use <code>BashOperator</code> to run a single command.</p>
<p>By following the five steps from the previous page, the DAG definition looks like the below:</p>
<p>Create a file named <code>1_first_dag.py</code> that contains the following code:</p>
<pre><code class="language-python"># Step-1: Import Python modules
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash_operator import BashOperator

# Step-2: Define default args
default_args = {
    &quot;owner&quot;: &quot;airflow&quot;,
    &quot;depends_on_past&quot;: False,
    &quot;retries&quot;: 1,
    &quot;retry_delay&quot;: timedelta(minutes=5),
    &quot;email&quot;: [&quot;airflow@example.com&quot;],
    &quot;email_on_failure&quot;: False,
    &quot;email_on_retry&quot;: False,
}

# Step-3: Instantiate DAG --- or creating a DAG object
dag = DAG(
    &quot;1_first_dag&quot;,
    description=&quot;first dag&quot;,
    schedule_interval=&quot;0 3 * * *&quot;,
    start_date=datetime(2022, 2, 17),
    catchup=False,
    tags=[&quot;custom&quot;],
)

# Step-4: Define Tasks
start = BashOperator(
    task_id=&quot;start&quot;,
    bash_command='echo &quot;start&quot;',
    dag=dag,
)

check_ip = BashOperator(
    task_id=&quot;check_ip&quot;,
    bash_command=&quot;curl checkip.amazonaws.com&quot;,
    dag=dag,
)

end = BashOperator(
    task_id=&quot;end&quot;,
    bash_command='echo &quot;stop&quot;',
    dag=dag,
)

# Step-5. Define task sequence and dependencies
start &gt;&gt; check_ip &gt;&gt; end
</code></pre>
<p><img src="build-airflow-pipelines-on-composer/write-the-dags/airflow-first-dag.png" alt="First dag" /></p>
<p>Note that we created a DAG object in this DAG, then associated it with every task in the pipeline. The DAG code can be verbose if you have many tasks in the pipeline.</p>
<p>To simplify it, Airflow provides another to define the DAG using <a href="https://docs.python.org/3/reference/datamodel.html#with-statement-context-managers">context managers</a>. Let's see how it works.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="context-manager-dag"><a class="header" href="#context-manager-dag">Context manager DAG</a></h1>
<p>DAGs can be used as <a href="https://docs.python.org/3/reference/datamodel.html#with-statement-context-managers">context managers</a> to assign each Operator/Sensor to that DAG automatically. This can be helpful if you have lots of tasks in a DAG; you don't need to repeat <code>dag=dag</code> in each Operator/Sensor. From the latest Airflow document, using context managers is recommended.</p>
<p>Below is a modified version of our first DAG in the previous page.</p>
<p>Create a file named <code>2_context_manager_dag.py</code> that contains the following code:</p>
<pre><code class="language-python">from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash_operator import BashOperator


default_args = {
    &quot;owner&quot;: &quot;airflow&quot;,
    &quot;depends_on_past&quot;: False,
    &quot;retries&quot;: 1,
    &quot;retry_delay&quot;: timedelta(minutes=5),
    &quot;email&quot;: [&quot;airflow@example.com&quot;],
    &quot;email_on_failure&quot;: False,
    &quot;email_on_retry&quot;: False,
}

with DAG(
    &quot;2_context_manager_dag&quot;,
    default_args=default_args,
    description=&quot;context manager Dag&quot;,
    schedule_interval=&quot;0 12 * * *&quot;,
    start_date=datetime(2021, 12, 1),
    catchup=False,
    tags=[&quot;custom&quot;],
) as dag:

    start = BashOperator(
        task_id=&quot;start&quot;,
        bash_command=&quot;echo start&quot;,
    )

    end = BashOperator(
        task_id=&quot;end&quot;,
        bash_command=&quot;echo stop&quot;,
    )

    start &gt;&gt; end
</code></pre>
<p>So far, in the two DAGs that we wrote, the tasks run one by one. It is excellent but may not be the most efficient. What if we have a pipeline, and there are some tasks that can be running in parallel?</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="parallel-tasks-dag"><a class="header" href="#parallel-tasks-dag">Parallel tasks DAG</a></h1>
<p>Let's assume that there are four tasks, T1, T2, T3, and T4, each one of them will take 10 seconds to run. Task T1, T2, T3 don't depend on others, while T4 depends on the success of T1, T2, and T3.</p>
<p>In an orchestration system that does not support parallel tasks, these tasks run sequentially one after another. It should take around 40 seconds to finish.</p>
<p><img src="build-airflow-pipelines-on-composer/write-the-dags/airflow-dag-sequential-tasks.png" alt="Sequential tasks" /></p>
<p>However, by design, Airflow supports concurrency, meaning that the tasks that don't depend on others can run in parallel.</p>
<p>The above DAG can be updated, and it should take ~20 seconds to finish.</p>
<p><img src="build-airflow-pipelines-on-composer/write-the-dags/airflow-dag-parallel-tasks.png" alt="Parallel tasks" /></p>
<p>The code of the DAG looks like this:</p>
<p>Create a file named <code>3_parallel_tasks_dag.py</code> that contains the following code:</p>
<pre><code class="language-python">from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash_operator import BashOperator


default_args = {
    &quot;owner&quot;: &quot;airflow&quot;,
    &quot;depends_on_past&quot;: False,
    &quot;retries&quot;: 1,
    &quot;retry_delay&quot;: timedelta(minutes=5),
    &quot;email&quot;: [&quot;airflow@example.com&quot;],
    &quot;email_on_failure&quot;: False,
    &quot;email_on_retry&quot;: False,
}

with DAG(
    &quot;3_parallel_tasks_dag&quot;,
    default_args=default_args,
    description=&quot;parallel tasks Dag&quot;,
    schedule_interval=&quot;0 12 * * *&quot;,
    start_date=datetime(2021, 12, 1),
    catchup=False,
    tags=[&quot;custom&quot;],
) as dag:

    t1 = BashOperator(
        task_id=&quot;T1&quot;,
        bash_command=&quot;echo T1&quot;,
    )

    t2 = BashOperator(
        task_id=&quot;T2&quot;,
        bash_command=&quot;echo T2&quot;,
    )

    t3 = BashOperator(
        task_id=&quot;T3&quot;,
        bash_command=&quot;echo T3&quot;,
    )

    t4 = BashOperator(
        task_id=&quot;T4&quot;,
        bash_command=&quot;echo T4&quot;,
    )

    # t1 &gt;&gt; t2 &gt;&gt; t3 &gt;&gt; t4
    [t1, t2, t3] &gt;&gt; t4
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="dynamic-tasks-dag"><a class="header" href="#dynamic-tasks-dag">Dynamic tasks DAG</a></h1>
<p>As mentioned before, in Airflow, a DAG is defined by Python code. There is no need for it to be purely declarative; you are free to use loops, functions, and more to define your DAG.</p>
<p>For example, let's say that there is a requirement to create a data pipeline to export data from tables in a database. Since new tables can be added at any time, the number of tables that we need to export data from is dynamic.</p>
<p>Firstly, we save and upload a few SQL queries in the Cloud Storage bucket.</p>
<p><img src="build-airflow-pipelines-on-composer/write-the-dags/airflow-dynamic-dag-sql-files.png" alt="SQL files" /></p>
<p>Then, we can create a DAG that uses a for loop to define some tasks. In this example, <code>DummyOperator</code> is used for demonstration purpose. As per name, <code>DummyOperator</code> does nothing. The tasks that use it are evaluated by the scheduler but never processed by the executor.</p>
<p>Create a file named <code>4_dynamic_dag.py</code> that contains the following code:</p>
<pre><code class="language-python">from datetime import datetime
from airflow import DAG
from airflow.operators.dummy_operator import DummyOperator

import os

sql_folder = os.path.abspath(os.path.join(os.path.dirname(__file__), &quot;sql&quot;))
sql_file_names = []


for file in os.listdir(sql_folder):
    if file.endswith(&quot;.sql&quot;):
        sql_file_names.append(file)

with DAG(
    &quot;4_dynamic_dag&quot;,
    description=&quot;my dynamic DAG&quot;,
    schedule_interval=&quot;0 12 * * *&quot;,
    template_searchpath=sql_folder,
    start_date=datetime(2021, 12, 1),
    catchup=False,
    tags=[&quot;custom&quot;],
) as dag:
    d1 = DummyOperator(task_id=&quot;kick_off_dag&quot;)
    d3 = DummyOperator(task_id=&quot;finish_dag&quot;)

    for i in sql_file_names:
        d2 = DummyOperator(task_id=f&quot;export_data_for_{i}&quot;)
        d1 &gt;&gt; d2 &gt;&gt; d3
</code></pre>
<p><img src="build-airflow-pipelines-on-composer/write-the-dags/airflow-dynamic-dag.png" alt="Dynamic dag" /></p>
<p>Dynamic DAGs are useful. However, try and keep the topology (the layout) of the DAG tasks relatively stable; dynamic DAGs are usually better used for dynamically loading configuration options or changing operator options.</p>
<p>Remember that readability is also important in designing Airflow DAGs.</p>
<p>Next, let's create a DAG to handle branching tasks.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="branching-dag"><a class="header" href="#branching-dag">Branching DAG</a></h1>
<p>When designing data pipelines, there may be use cases that require more complex task flows than <em>Task A &gt; Task B &gt; Task C</em>. For example, let's say that there is a use case where different tasks need to be chosen to execute based on the results of an upstream task. We call this <code>branching</code> in Airflow, and it uses a particular Operator <code>BranchPythonOperator</code> to handle this use case.</p>
<p>The <code>BranchPythonOperator</code> takes a Python function as an input. The function must return a list of task IDs for the DAG to process using the function.</p>
<p>To give you an example of branching, let's create a DAG that uses Python <a href="https://docs.python.org/3/library/random.html#random.choice">random.choice</a> function to decide which type of no-op transform it will execute.</p>
<p>Create a file named <code>5_branching_dag.py</code> that contains the following code:</p>
<pre><code class="language-python">from datetime import datetime
import time

from airflow.models import DAG
from airflow.operators.dummy import DummyOperator
from airflow.operators.python import BranchPythonOperator

with DAG(
    dag_id=&quot;5_braching_dag&quot;,
    start_date=datetime(2021, 12, 1),
    catchup=False,
    schedule_interval=&quot;@daily&quot;,
    tags=[&quot;custom&quot;],
) as dag:
    get_source = DummyOperator(task_id=&quot;get_source&quot;)

    def branch_func():
        if int(time.time()) % 2 == 0:
            return &quot;even_number_transform&quot;
        else:
            return &quot;odd_number_transform&quot;

    branch_check = BranchPythonOperator(
        task_id=&quot;branch_check&quot;, python_callable=branch_func
    )
    even_number_transform = DummyOperator(task_id=&quot;even_number_transform&quot;)
    odd_number_transform = DummyOperator(task_id=&quot;odd_number_transform&quot;)

    get_source &gt;&gt; branch_check &gt;&gt; [even_number_transform, odd_number_transform]
</code></pre>
<p><img src="build-airflow-pipelines-on-composer/write-the-dags/airflow-branching-dag.png" alt="Branching dag" /></p>
<p>From the historical runs, we can see that different transform tasks were run.</p>
<p><img src="build-airflow-pipelines-on-composer/write-the-dags/airflow-branching-dag-results.png" alt="Branching dag runs" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="xcoms-dag"><a class="header" href="#xcoms-dag">Xcoms DAG</a></h1>
<p>Sharing data between Tasks is a common use case in Airflow. For example, a Task calls an API to get the data filenames for today's data ingestion DAG. The following Tasks need to know these filenames to load the data.</p>
<p>XCom (short for cross-communication) is a native feature within Airflow. XComs allow tasks to exchange Task metadata or small amounts of data. XComs can be &quot;pushed&quot; (sent) or &quot;pulled&quot; (retrieved). When a task pushes an XCom, it makes it generally available to other tasks.</p>
<p>There are two ways to push a value to XCom.</p>
<ol>
<li>Use <code>xcom_pull</code></li>
<li>Return the value in your function, and it will be pushed to Xcom automatically.</li>
</ol>
<p>When a Task (An instance of an Operator) is running, it will get a copy of the Task Instance passed to it. When <code>python_callable</code> is used inside a <code>PythonOperator</code> Task, you can get the task instance object via <code>ti = kwargs['ti']</code>. After that, we can call the <code>xcom_pull</code> function to retrieve the Xcom value.</p>
<p>Let's create a DAG to exchange value between tasks.</p>
<p>Create a file named <code>6_xcoms_dag.py</code> that contains the following code:</p>
<pre><code class="language-python">from datetime import datetime
from airflow.models import DAG
from airflow.operators.python_operator import PythonOperator

DAG = DAG(
    dag_id=&quot;6_xcoms_dag&quot;,
    start_date=datetime.now(),
    catchup=False,
    schedule_interval=&quot;@once&quot;,
    tags=[&quot;custom&quot;],
)


def push_function(**kwargs):
    ls = [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;]
    return ls


push_task = PythonOperator(
    task_id=&quot;push_task&quot;, python_callable=push_function, provide_context=True, dag=DAG
)


def pull_function(**kwargs):
    ti = kwargs[&quot;ti&quot;]
    ls = ti.xcom_pull(task_ids=&quot;push_task&quot;)
    print(ls)


pull_task = PythonOperator(
    task_id=&quot;pull_task&quot;, python_callable=pull_function, provide_context=True, dag=DAG
)

push_task &gt;&gt; pull_task
</code></pre>
<p><img src="build-airflow-pipelines-on-composer/write-the-dags/airflow-xcoms-dag.png" alt="Xcoms dag" /></p>
<p>From Airflow UI, there is a tab next to <code>Log</code> called XCom that shows XCom values.</p>
<p><img src="build-airflow-pipelines-on-composer/write-the-dags/airflow-xcoms-push.png" alt="Xcoms push" /></p>
<p>Let's check the <code>pull_task</code>. Yes, the value was retrieved.</p>
<p><img src="build-airflow-pipelines-on-composer/write-the-dags/airflow-xcoms-pull.png" alt="Xcoms pull" /></p>
<p>XCom values are stored in Airflow database and are shown on UI or logs. It is important not to store sensitive information and large data in them.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="pythonoperator-and-taskflow-dag"><a class="header" href="#pythonoperator-and-taskflow-dag">PythonOperator and TaskFlow Dag</a></h1>
<p>Tasks that use <code>PythonOperator</code> execute Python callables/functions. You can pass parameters to the function via the op_kwargs parameter.</p>
<p>Airflow 2.0 adds a new style of authoring dags called the <a href="https://airflow.apache.org/docs/apache-airflow/2.0.0/concepts.html#taskflow-api">TaskFlow API</a> which removes a lot of the boilerplate around creating <code>PythonOperator</code>, managing dependencies between task and accessing XCom values.</p>
<p>Let's create a DAG that uses both <code>PythonOperator</code> and TaskFlow API to show how to create tasks using Python functions.</p>
<p>In the below DAG, the first task uses <code>PythonOperator</code> to print the task context, including the parameter (<code>my_keyword</code>) that is passed in. The second task and third tasks are created using TaskFlow decorator. These tasks run Python functions without using <code>PythonOperator</code>.</p>
<p>Create a file named <code>7_python_operator_and_taskflow_dag.py</code> that contains the following code:</p>
<pre><code class="language-python">import time
from datetime import datetime
from pprint import pprint

from airflow import DAG
from airflow.decorators import task
from airflow.operators.python import PythonOperator


with DAG(
    dag_id=&quot;7_python_operator_and_taskflow_dag&quot;,
    schedule_interval=None,
    start_date=datetime(2021, 12, 1),
    catchup=False,
    tags=[&quot;custom&quot;],
) as dag:
    # 1. print context using PythonOperator
    def print_context(ds, **kwargs):
        &quot;&quot;&quot;Print the Airflow context and ds variable from the context.&quot;&quot;&quot;
        pprint(kwargs)
        print(kwargs[&quot;my_keyword&quot;])
        print(ds)
        return &quot;Whatever you return gets printed in the logs&quot;

    print_the_context = PythonOperator(
        task_id=&quot;print_the_context&quot;,
        python_callable=print_context,
        op_kwargs={&quot;my_keyword&quot;: &quot;Airflow&quot;},
    )

    # 2. sleep task using TaskFlow decorator
    @task(task_id=&quot;sleep_for_5&quot;)
    def my_sleeping_function():
        &quot;&quot;&quot;This is a function that will run within the DAG execution&quot;&quot;&quot;
        time.sleep(5)

    sleeping_task = my_sleeping_function()

    # 3. print context again using TaskFlow decorator
    @task(task_id=&quot;print_the_context_again&quot;)
    def print_context_again(ds=None, **kwargs):
        &quot;&quot;&quot;Print the Airflow context and ds variable from the context.&quot;&quot;&quot;
        pprint(kwargs)
        print(kwargs[&quot;my_keyword&quot;])
        print(ds)
        return &quot;Whatever you return gets printed in the logs&quot;

    print_the_context_again = print_context_again(my_keyword=&quot;Airflow&quot;)

    print_the_context &gt;&gt; sleeping_task &gt;&gt; print_the_context_again
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="custom-operator-dag"><a class="header" href="#custom-operator-dag">Custom operator DAG</a></h1>
<p>One thing that makes Airflow so popular is its built-in modules. There are built-in modules connecting to three major public cloud providers, as well as popular services and tools. When writing Airflow pipelines, it is always recommended to use built-in modules whenever they are available.</p>
<p>However, there are occasions when built-in modules cannot fulfill the requirements. Fortunately, Airflow has a plugin manager built-in that can integrate external features to its core by dropping files in your <code>$AIRFLOW_HOME/plugins</code> folder.</p>
<p>In this chapter, we will create a basic DAG that uses a custom Operator.</p>
<p>Firstly, let's create a <code>HelloOperator</code>. It inherits the <code>BaseOperator</code> and overrides the <code>execute</code> method. It also takes a parameter called <code>operator_param</code>.</p>
<p>Create a file named <code>hello_operator.py</code> that contains the following code:</p>
<pre><code class="language-python">from airflow.models import BaseOperator
from airflow.utils.decorators import apply_defaults


class HelloOperator(BaseOperator):
    @apply_defaults
    def __init__(self, operator_param, *args, **kwargs):
        self.operator_param = operator_param
        super(HelloOperator, self).__init__(*args, **kwargs)

    def execute(self, context):
        greeting = f&quot;Hello, {self.operator_param}!&quot;
        self.log.info(greeting)
        return greeting
</code></pre>
<p>Then we can create a DAG that uses the custom operator <code>HelloOperator</code>.</p>
<p>Create a file named <code>8_custom_operator_dag.py</code> that contains the following code:</p>
<pre><code class="language-python">from datetime import datetime
from airflow import DAG
from airflow.operators.dummy_operator import DummyOperator
from operators.hello_operator import HelloOperator


dag = DAG(
    &quot;8_custom_operator_dag&quot;,
    schedule_interval=&quot;0 12 * * *&quot;,
    start_date=datetime(2021, 12, 1),
    catchup=False,
    tags=[&quot;custom&quot;],
)

start = DummyOperator(
    task_id=&quot;start&quot;,
    dag=dag,
)

hello = HelloOperator(
    task_id=&quot;hello&quot;,
    operator_param=&quot;composer tutorial&quot;,
    dag=dag,
)

end = DummyOperator(
    task_id=&quot;end&quot;,
    dag=dag,
)

start &gt;&gt; hello &gt;&gt; end
</code></pre>
<p>To run this DAG in Cloud Composer, the custom Operator file needs to be uploaded to the <code>$AIRFLOW_HOME/plugins</code> folder. In the context of Cloud Composer, it is the <code>plugins</code> folder under the Cloud Storage bucket.</p>
<p><img src="build-airflow-pipelines-on-composer/write-the-dags/composer-plugins-folder.png" alt="Composer plugin folder" /></p>
<p>Finally, we can run the DAG. From the log, we can see that it successfully logs the information.</p>
<p><img src="build-airflow-pipelines-on-composer/write-the-dags/airflow-custom-operator-dag.png" alt="Custom operator DAG" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="case-study-generate-nudges"><a class="header" href="#case-study-generate-nudges">Case study: generate nudges</a></h1>
<p>From the previous chapters, we have learned the Airflow concepts and how to write DAGs and custom Operators. In this chapter, let's work on a real-world use case and create an Airflow pipeline together.</p>
<h2 id="background"><a class="header" href="#background">Background</a></h2>
<p>An e-commerce company (let's call it <code>Cell-mate</code>) is a famous online shop selling phones and accessories. They have various data sources which export data in CSV files to a <em>Google Cloud Storage</em> (GCS) bucket on a daily basis.</p>
<h2 id="data-sources"><a class="header" href="#data-sources">Data sources</a></h2>
<p>The <code>Cell-mate</code> has three primary data sources:</p>
<ul>
<li>Accounts—It contains all the information from the accounts of their customers.</li>
<li>Items—It contains all the items that are listed on the <!-- textlint-disable terminology -->website<!-- textlint-enable -->.</li>
<li>Activities—It contains all the viewer activities from the customers.</li>
</ul>
<h2 id="goal"><a class="header" href="#goal">Goal</a></h2>
<p>The product team in <code>Cell-mate</code> would like to build a pipeline to generate nudge emails for the customers who recently viewed the items on their <!-- textlint-disable terminology -->website<!-- textlint-enable --> but didn't make the purchase. As the first step, they would like the nudge data to be stored in a place so that the email campaign service can use it.</p>
<p>Let's continue and create the DAG first.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="create-nudges-dag"><a class="header" href="#create-nudges-dag">Create nudges DAG</a></h1>
<p>When designing a DAG, we often start with critical tasks such as loading the data, transformation, exporting the data, etc. Then iteratively, we add other tasks such as checking if the BigQuery Dataset exists or not, if today's load has happened, etc.</p>
<h2 id="key-tasks"><a class="header" href="#key-tasks">Key tasks</a></h2>
<p>To generate the nudges for the customers, we can load the daily exported CSV files to three tables in BigQuery. After that, we can run a SQL query that joins the three tables, create the nudge information, and store the results in another table.
The DAG looks like this:</p>
<p><img src="case-study/case-study-dag-1.png" alt="A diagram of the case study DAG" /></p>
<p>As the CSV files are on <em>Google Cloud Storage</em> (GCS) and we need to load them to BigQuery, we need an Operator that can do GCS to BigQuery. GCS to BigQuery is a pretty generic job, let's search in <a href="https://registry.astronomer.io/">Astronomer Registry</a> to see if Airflow has it in the built-in libraries:</p>
<p><img src="case-study/GCS-to-BQ-search.png" alt="Screenshot showing the search UI" /></p>
<p>Yes, we found the <code>GCSToBigQueryOperator</code>. Following it's <a href="https://registry.astronomer.io/providers/google/modules/gcstobigqueryoperator/#example-dags">documentation</a>, let's create our three data load tasks:</p>
<p>Create a file named <code>9_generate_nudges_dag.py</code> that contains the following code:</p>
<pre><code class="language-python">    load_accounts_csv = GCSToBigQueryOperator(
        task_id=&quot;load_accounts_csv&quot;,
        bucket=STORE_RAW_DATA_BUCKET,
        source_objects=[f&quot;accounts_{CURRENT_DATE}.csv&quot;],
        destination_project_dataset_table=f&quot;{DATASET_ID}.accounts&quot;,
        schema_fields=[
            {&quot;name&quot;: &quot;account_id&quot;, &quot;type&quot;: &quot;INTEGER&quot;, &quot;mode&quot;: &quot;NULLABLE&quot;},
            {&quot;name&quot;: &quot;account_name&quot;, &quot;type&quot;: &quot;STRING&quot;, &quot;mode&quot;: &quot;NULLABLE&quot;},
            {&quot;name&quot;: &quot;email&quot;, &quot;type&quot;: &quot;STRING&quot;, &quot;mode&quot;: &quot;NULLABLE&quot;},
        ],
        write_disposition=&quot;WRITE_TRUNCATE&quot;,
    )

    load_activities_csv = GCSToBigQueryOperator(
        task_id=&quot;load_activities_csv&quot;,
        bucket=STORE_RAW_DATA_BUCKET,
        source_objects=[f&quot;activities_{CURRENT_DATE}.csv&quot;],
        destination_project_dataset_table=f&quot;{DATASET_ID}.activities&quot;,
        schema_fields=[
            {&quot;name&quot;: &quot;account_id&quot;, &quot;type&quot;: &quot;INTEGER&quot;, &quot;mode&quot;: &quot;NULLABLE&quot;},
            {&quot;name&quot;: &quot;item_id&quot;, &quot;type&quot;: &quot;INTEGER&quot;, &quot;mode&quot;: &quot;NULLABLE&quot;},
            {&quot;name&quot;: &quot;visit_time&quot;, &quot;type&quot;: &quot;TIMESTAMP&quot;, &quot;mode&quot;: &quot;NULLABLE&quot;},
        ],
        write_disposition=&quot;WRITE_TRUNCATE&quot;,
    )

    load_items_csv = GCSToBigQueryOperator(
        task_id=&quot;load_items_csv&quot;,
        bucket=STORE_RAW_DATA_BUCKET,
        source_objects=[f&quot;items_{CURRENT_DATE}.csv&quot;],
        destination_project_dataset_table=f&quot;{DATASET_ID}.items&quot;,
        schema_fields=[
            {&quot;name&quot;: &quot;item_id&quot;, &quot;type&quot;: &quot;INTEGER&quot;, &quot;mode&quot;: &quot;NULLABLE&quot;},
            {&quot;name&quot;: &quot;item_name&quot;, &quot;type&quot;: &quot;STRING&quot;, &quot;mode&quot;: &quot;NULLABLE&quot;},
            {&quot;name&quot;: &quot;price&quot;, &quot;type&quot;: &quot;FLOAT&quot;, &quot;mode&quot;: &quot;NULLABLE&quot;},
        ],
        write_disposition=&quot;WRITE_TRUNCATE&quot;,
    )

    generate_nudges = GenerateNudgesOperator(
        task_id=&quot;generate_nudges&quot;,
        nudges_query=NUDGES_QUERY,
        destination_dataset_table=&quot;analytics.nudges&quot;,
    )
</code></pre>
<p>The <code>generate_nudges</code> task runs a BigQuery query and saves the results in a BigQuery table. It may look like an ordinary job. But unfortunately, there isn't an existing Operator that does the job. We need to create a custom Operator. In this custom Operator, we can use the built-in <code>BigQueryHook</code> to interact with BigQuery. We can find its source code on <a href="https://github.com/apache/airflow/blob/main/airflow/providers/google/cloud/hooks/bigquery.py#L66">GitHub</a>.</p>
<p>Let's call the custom Operator <code>GenerateNudgesOperator</code></p>
<p>Create a file named <code>generate_nudges_operator.py</code> that contains the following code:</p>
<pre><code class="language-python">from airflow.models import BaseOperator
from airflow.providers.google.cloud.hooks.bigquery import BigQueryHook
from airflow.utils.decorators import apply_defaults


class GenerateNudgesOperator(BaseOperator):
    @apply_defaults
    def __init__(
        self,
        *,
        nudges_query: str,
        destination_dataset_table: str,
        gcp_conn_id: str = &quot;google_cloud_default&quot;,
        **kwargs,
    ) -&gt; None:
        super().__init__(**kwargs)

        self.gcp_conn_id = gcp_conn_id
        self.nudges_query = nudges_query
        self.destination_dataset_table = destination_dataset_table

    def execute(self, context):
        self.log.info(
            f&quot;Generating nudges with query {self.nudges_query} and write into {self.destination_dataset_table}...&quot;
        )

        hook = BigQueryHook(gcp_conn_id=self.gcp_conn_id, use_legacy_sql=False)

        hook.run_query(
            sql=self.nudges_query,
            destination_dataset_table=self.destination_dataset_table,
            write_disposition=&quot;WRITE_TRUNCATE&quot;,
        )
</code></pre>
<p>And the task looks like this:</p>
<pre><code class="language-python">    generate_nudges = GenerateNudgesOperator(
        task_id=&quot;generate_nudges&quot;,
        nudges_query=NUDGES_QUERY,
        destination_dataset_table=&quot;analytics.nudges&quot;,
    )
</code></pre>
<h2 id="add-other-tasks"><a class="header" href="#add-other-tasks">Add other tasks</a></h2>
<h3 id="check-bigquery-dataset"><a class="header" href="#check-bigquery-dataset">Check BigQuery Dataset</a></h3>
<p>As a safety measure, we should add a task to check if the BigQuery Dataset exists before loading the CSV files.</p>
<p>After checking the Airflow built-in Operators, there isn't a built-in one. Let's create another custom Operator called <code>CheckBigQueryDatasetOperator</code>. Because it needs to access BigQuery, we can use the <code>BigQueryHook</code> again.</p>
<p>Create a file named <code>check_bigquery_dataset_operator.py</code> that contains the following code:</p>
<pre><code class="language-python">from airflow.models import BaseOperator
from airflow.providers.google.cloud.hooks.bigquery import BigQueryHook
from airflow.utils.decorators import apply_defaults


class CheckBigQueryDatasetOperator(BaseOperator):
    @apply_defaults
    def __init__(
        self,
        *,
        dataset_id,
        gcp_conn_id=&quot;google_cloud_default&quot;,
        **kwargs,
    ) -&gt; None:
        super().__init__(**kwargs)

        self.gcp_conn_id = gcp_conn_id
        self.dataset_id = dataset_id

    def execute(self, context):
        self.log.info(f&quot;Checking dataset {self.dataset_id}...&quot;)

        hook = BigQueryHook(
            gcp_conn_id=self.gcp_conn_id,
        )

        # Check dataset exists
        datasets_list = hook.get_datasets_list()
        self.log.info(f&quot;datasets_list: {datasets_list}&quot;)
        is_dataset_existed = False
        if datasets_list:
            datasets_id_list = [dataset.dataset_id for dataset in datasets_list]
            is_dataset_existed = self.dataset_id in datasets_id_list

        if is_dataset_existed:
            return True
        else:
            raise Exception(f&quot;Dataset id {self.dataset_id} not found&quot;)
</code></pre>
<p>And the task looks like this:</p>
<pre><code class="language-python">    check_bigquery_dataset = CheckBigQueryDatasetOperator(
        task_id=&quot;check_bigquery_dataset&quot;,
        dataset_id=DATASET_ID,
    )
</code></pre>
<p>Now the DAG looks like this:</p>
<p><img src="case-study/case-study-dag-2.png" alt="A diagram of the changed DAG" /></p>
<h3 id="avoid-duplicated-runs"><a class="header" href="#avoid-duplicated-runs">Avoid duplicated runs</a></h3>
<p>It is always a good idea to check if today's run finishes. There are multiple ways to do that. If there are numerous data pipelines, you should build an API to record their runs. When a pipeline is kicked off, it checks if the run for that day has been finished by calling the API. If yes, this particular DAG run should not continue with the following tasks.</p>
<p>In our case study, as we only have one data pipeline, we can assume that:</p>
<ul>
<li>The email system sends the emails out every day</li>
<li>The data pipeline uploads a file to GCS (using the current date as the filename)</li>
</ul>
<p>With this assumption, a few tasks can be added to finalize the DAG.</p>
<p><img src="case-study/case-study-dag-3.png" alt="A diagram of the final DAG" /></p>
<p>Let's dig into the new tasks.</p>
<h4 id="get-latest-run-date"><a class="header" href="#get-latest-run-date">Get latest run date</a></h4>
<p>As mentioned above, we can check if a file named as current date exists in the GCS bucket. To do this, we can use the <code>GCSListObjectsOperator</code> from Airflow built-in libraries. The task looks like this:</p>
<pre><code class="language-python">    get_latest_run_date = GCSListObjectsOperator(
        task_id=&quot;get_latest_run_date&quot;,
        bucket=NUDGES_HISTORY_BUCKET,
        prefix=CURRENT_DATE,
    )
</code></pre>
<p>From the <a href="https://github.com/apache/airflow/blob/main/airflow/providers/google/cloud/operators/gcs.py#L279">code</a> on Github, it returns the URI of the file on GCS or empty array(if the file does not exist). In Airflow Operator, any value that is returned by <code>execute</code> function is stored in <code>xcom</code>.</p>
<p>Now we have a value in <code>xcom</code>, let's move on to the next task.</p>
<h4 id="check-run-date"><a class="header" href="#check-run-date">Check run date</a></h4>
<p>In this task, we can use <code>BranchPythonOperator</code> to decide if this particular run should continue. The task looks like this:</p>
<pre><code class="language-python">    check_run_date = BranchPythonOperator(
        task_id=&quot;check_run_date&quot;, python_callable=branch_func
    )
</code></pre>
<p>And the <code>branch_func</code>:</p>
<pre><code class="language-python">def branch_func(ti):
    # ti means Task Instance here. It is used to retrieve Xcom value
    xcom_return_value = ti.xcom_pull(task_ids=&quot;get_latest_run_date&quot;, key=&quot;return_value&quot;)
    log.info(f&quot;Xcom_return_value: {xcom_return_value}&quot;)
    if xcom_return_value:
        log.info(&quot;Today's run already exists, finishing the dag...&quot;)
        return &quot;finish_run&quot;
    else:
        log.info(&quot;Today's run does not exist, kicking off the run...&quot;)
</code></pre>
<p>From here, Airflow will decide if the DAG run should continue loading data or finish. To make the pipeline more user-friendly, we can use two <code>DummyOperator</code> to represent <code>kick_off_run</code> and <code>finish_run</code> tasks.</p>
<pre><code class="language-python">    kick_off_run = DummyOperator(task_id=&quot;kick_off_run&quot;)
    finish_run = DummyOperator(task_id=&quot;finish_run&quot;)
</code></pre>
<h2 id="summary-1"><a class="header" href="#summary-1">Summary</a></h2>
<p>In this chapter, we've walked through designing and creating a DAG to load data from three CSV files and generate nudges in BigQuery. If you'd like to run it, you can run it yourself, be sure to replace the project ID and buckets in the DAG file.</p>
<pre><code class="language-python">PROJECT_ID = &quot;derrick-sandbox&quot;
NUDGES_HISTORY_BUCKET = &quot;nudges_history&quot;
STORE_RAW_DATA_BUCKET = &quot;store_raw_data&quot;
</code></pre>
<p>Or you can wait until the next chapter, in which I will cover the testing strategy, including a quick end-to-end test that can generate test files and trigger the DAG.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="testing"><a class="header" href="#testing">Testing</a></h1>
<p>Like any software, Airflow pipelines need to be tested. In Airflow, we usually do unit tests and end-to-end tests to ensure Airflow pipelines work well before deployments.</p>
<p>In our project, <a href="https://docs.pytest.org/">Pytest</a> is used as the test runner.</p>
<h2 id="unit-tests"><a class="header" href="#unit-tests">Unit tests</a></h2>
<p>When testing Airflow, the first thing we'd like to do is to use the below test to make sure all the DAGs to be deployed don't have errors themselves. These include syntax errors, library import errors, etc.</p>
<pre><code class="language-python">from airflow.models import DagBag


def test_dag_loaded():
    dag_bag = DagBag(include_examples=False)
    assert dag_bag.import_errors == {}
</code></pre>
<p>If there are custom Operators, Hooks, and Sensors, they also need to be unit tested.</p>
<p>For example, with the <code>BigQueryHook</code> being mocked, the <code>CheckBigQueryDatasetOperator</code> can be tested like as the below:</p>
<pre><code class="language-python">import unittest
from unittest import mock
import pytest

from operators.check_bigquery_dataset_operator import CheckBigQueryDatasetOperator
from google.cloud.bigquery.dataset import DatasetListItem


DUMMY_DATASET_ID = &quot;dummy_dataset_id&quot;
DUMMY_NON_EXISTED_DATASET_ID = &quot;dummy_non_existed_dataset_id&quot;
DUMMY_DATASETS = [
    {
        &quot;kind&quot;: &quot;bigquery#dataset&quot;,
        &quot;location&quot;: &quot;US&quot;,
        &quot;id&quot;: &quot;your-project:dummy_dataset_id&quot;,
        &quot;datasetReference&quot;: {
            &quot;projectId&quot;: &quot;your-project&quot;,
            &quot;datasetId&quot;: &quot;dummy_dataset_id&quot;,
        },
    },
    {
        &quot;kind&quot;: &quot;bigquery#dataset&quot;,
        &quot;location&quot;: &quot;US&quot;,
        &quot;id&quot;: &quot;your-project:another_dummy_dataset_id&quot;,
        &quot;datasetReference&quot;: {
            &quot;projectId&quot;: &quot;your-project&quot;,
            &quot;datasetId&quot;: &quot;another_dummy_dataset_id&quot;,
        },
    },
]


class TestCheckBigQueryDatasetOperator(unittest.TestCase):
    @mock.patch(&quot;operators.check_bigquery_dataset_operator.BigQueryHook&quot;)
    def test_existed_dataset(self, mock_hook):
        operator = CheckBigQueryDatasetOperator(
            task_id=&quot;dataset_exists_task&quot;,
            dataset_id=DUMMY_DATASET_ID,
        )

        mock_hook.return_value.get_datasets_list.return_value = [
            DatasetListItem(d) for d in DUMMY_DATASETS
        ]
        assert operator.execute(None) == True

    @mock.patch(&quot;operators.check_bigquery_dataset_operator.BigQueryHook&quot;)
    def test_non_existed_dataset(self, mock_hook):
        operator = CheckBigQueryDatasetOperator(
            task_id=&quot;dataset_exists_task&quot;,
            dataset_id=DUMMY_NON_EXISTED_DATASET_ID,
        )

        mock_hook.return_value.get_datasets_list.return_value = [
            DatasetListItem(d) for d in DUMMY_DATASETS
        ]

        with pytest.raises(
            Exception, match=f&quot;Dataset id {DUMMY_NON_EXISTED_DATASET_ID} not found&quot;
        ):
            operator.execute(None)
</code></pre>
<h2 id="end-to-end-tests"><a class="header" href="#end-to-end-tests">End-to-end tests</a></h2>
<p>With the unit tests ensuring DAGs and custom plugins are reasonable, we also need some end-to-end tests to ensure the pipeline runs well.</p>
<p>Like any other end-to-end tests, we generate some input, run the program, and check the output. To test our DAG, the test steps look like the below:</p>
<ol>
<li>Create and upload three CSV files to <em>Google Cloud Storage</em> (GCS)</li>
<li>Trigger Airflow DAG</li>
<li>Wait for 30 seconds for the DAG to finish</li>
<li>Check the nudge table is refreshed</li>
</ol>
<p>Ideally, this test should be running in a test environment with a test Composer environment. Then we can trigger the DAG using the Airflow <a href="https://en.wikipedia.org/wiki/Representational_state_transfer">REST</a> API by following the Google document <a href="https://cloud.google.com/composer/docs/access-airflow-api">here</a>.</p>
<p>I understand that there may be expensive to host a test Composer because it needs to run 24/7. To save the cost, you can run Airflow locally (I will cover more details of this approach in the another chapter) with <code>docker-compose</code> and using the below code to trigger the DAG:</p>
<pre><code class="language-python">    def trigger_dag(self):
        subprocess.run(
            &quot;docker-compose run airflow-scheduler airflow dags trigger 9_generate_nudges_dag&quot;,
            shell=True,
        )
</code></pre>
<p>Now we have the tests ready, let's move on to the next topic - CI and CD.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ci-and-cd"><a class="header" href="#ci-and-cd">CI and CD</a></h1>
<p>At this point, we have the DAG and plugins ready and tested. What's next? You can upload them manually to Composer and have a working environment. However, in a proper Airflow project, we should have CI (continuous integration) and CD (continuous deployment) so that the development and operation activities can be smoother.</p>
<h2 id="ci"><a class="header" href="#ci">CI</a></h2>
<p>Typical CI tasks in Airflow pipelines include:</p>
<ul>
<li><em>Lint</em>—Highlights syntactical and stylistic problems in the Airflow pipeline Python code, which often helps you identify and correct subtle programming errors or unconventional coding practices that can lead to errors.</li>
<li><em>Unit tests</em>—Runs the unit tests to ensure DAGs and plugins are working as expected.</li>
<li><em>End-to-end tests</em>—Makes sure the DAG works with all other integrated systems.</li>
</ul>
<h2 id="cd"><a class="header" href="#cd">CD</a></h2>
<p>In the CD tasks, DAGs and plugins are uploaded to Composer. There may be another task to send Slack or email notifications.</p>
<p>There are multiple ways to upload DAGs to Composer. Google created a <a href="https://cloud.google.com/composer/docs/how-to/using/managing-dags">guide</a> that shows different ways. However, I prefer to work with the Cloud Storage bucket directly using <code>gsutil rsync</code> to sync the DAG folder in my repository to the DAG folder on <em>Google Cloud Storage</em> (GCS). In this way, I don't need to think about if any DAG file should be deleted or not. The DAGs on Cloud Storage match what are in the repository.</p>
<pre><code class="language-bash">gsutil rsync dags gs://my-composer-bucket/dags
</code></pre>
<p>To upload the plugin files, I use <code>gsutil rsync</code> to sync them to the plugins folder on GCS.</p>
<pre><code class="language-bash">gsutil rsync plugins gs://my-composer-bucket/plugins
</code></pre>
<p>We've covered all the information in this case study. Remember that I told you there is a way to run Airflow locally? Let's do it in the next chapter.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="set-up-and-run-airflow-locally"><a class="header" href="#set-up-and-run-airflow-locally">Set up and run Airflow locally</a></h1>
<p>There are a few ways to run Airflow locally:</p>
<ol>
<li>Run it in a Python virtual environment</li>
<li>Run it using <code>docker-compose</code></li>
<li>Deploy it using Helm Chart</li>
</ol>
<h2 id="python-virtual-environment-deployment"><a class="header" href="#python-virtual-environment-deployment">Python virtual environment deployment</a></h2>
<p>Airflow community created a <a href="https://airflow.apache.org/docs/apache-airflow/stable/start/local.html">guide</a> that shows how to run Airflow in a virtual environment.</p>
<p>This setup is lightweight and suitable to test something quickly. It is not a production-ready setup because it only processes tasks sequentially.</p>
<h2 id="docker-compose"><a class="header" href="#docker-compose">Docker-compose</a></h2>
<p>Using <code>docker-compose</code> is the preferred way to run Airflow locally. Again, the Airflow community is kindly created a <a href="https://airflow.apache.org/docs/apache-airflow/stable/start/docker.html">guide</a> including a pre-baked <code>docker-compose.yaml</code> file.</p>
<p>When you do <code>docker-compose up</code>, a whole Airflow cluster is up, including:</p>
<!-- textlint-disable rousseau -->
<ul>
<li><code>airflow-scheduler</code>—The scheduler monitors all tasks and DAGs, then triggers the task instances once their dependencies are complete.</li>
<li><code>airflow-webserver</code>—The webserver is available at <a href="http://localhost:8080/">http://localhost:8080</a>.</li>
<li><code>airflow-worker</code>—The worker that executes the tasks given by the scheduler.</li>
<li><code>airflow-init</code>—The initialization service.</li>
<li><code>flower</code>—The flower app for monitoring the environment. It is available at <a href="http://localhost:5555/">http://localhost:5555</a>.</li>
<li><code>postgres</code>—The database.</li>
<li><code>redis</code>—The redis is the broker that forwards messages from scheduler to worker.</li>
</ul>
<!-- textlint-enable -->
<p>All these services allow you to run Airflow with <code>CeleryExecutor</code>.</p>
<p>There is only one trick/bug that you should be aware of - Docker has this weird of creating volumes with root user. When running Airflow with <code>docker-compose</code>, there is a <a href="https://github.com/helm/charts/issues/23589">GitHub issue</a> that includes a temp solution.
If you see errors after running <code>docker-comose up</code>: <code>Errno 13 - Permission denied: '/opt/airflow/logs/scheduler</code>, you need to stop the <code>docker-compose</code> and run <code>chmod -R 777 logs/</code>. You should be able to start your Airflow cluster again using <code>docker-comose up</code>.</p>
<h2 id="helm-chart"><a class="header" href="#helm-chart">Helm Chart</a></h2>
<p>How can we forget Kubernetes these days if we want to deploy something? To deploy Airflow to Kubernetes, <a href="https://airflow.apache.org/docs/helm-chart/stable/index.html">here</a> is a guide from the Airflow community.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="whats-next"><a class="header" href="#whats-next">What's next?</a></h1>
<!-- textlint-disable @textlint-rule/proselint -->
<p>Congratulations, you have finished this mini-book!</p>
<!-- textlint-enable -->
<p>If you are interested in learning more about Airflow and Composer, below are some extra resources:</p>
<ul>
<li><a href="https://airflow.apache.org/docs/">Airflow official documentation</a></li>
<li><a href="https://github.com/apache/airflow">Airflow open source project</a></li>
<li><a href="https://www.astronomer.io/guides/">Airflow guides from Astronomer</a></li>
<li><a href="https://cloud.google.com/composer/docs">Google Cloud Composer documentation</a></li>
<li><a href="https://www.cloudskillsboost.google/catalog">Google Cloud Skills Boost</a> (search Composer)</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="contributors"><a class="header" href="#contributors">Contributors</a></h1>
<p>This mini-book was created by <a href="https://github.com/derrickqin">Derrick Qin</a> from <a href="https://www.doit-intl.com/">DoiT International</a> and reviewed by <a href="https://github.com/doit-mattporter">Matthew Porter</a> and <a href="https://github.com/nomiro">Naomi Rose</a>.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
            </nav>

        </div>

        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
    </body>
</html>
